---
title: "PROJECT I: MovieLens"
author: "FH"
date: "September 30, 2020"
---output: html_document
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::knit_meta(clean=T)
```
# Inroduction

The analysis performed within this project is using analysis strategies of a recommendation system developed by the winners of the *Netflix challenge*[^1]. Since Netflix data is not publicly available the data used to develop a recommendation system is based on the **MovieLens** data. The original "MovieLens"(20M) data set was generated by the GroupLens[^2] research lab and can be found here:
&nbsp; 

- MovieLens for 20M dataset &nbsp;
https://grouplens.org/datasets/movielens/20m/ 
&nbsp;

- MovieLens for 10M dataset 
&nbsp;

https://grouplens.org/datasets/movielens/10m/ 
&nbsp;


The recommendations were developed for the **edx** data which is a subset of the **MovieLens** data. For the evaluation of the recommendation algorithm a **validation** data set was generated. The **validation** data set was only used in the final step to test the final algorithm and contained only 10% of **edx** data. The final model with the smallest RMSE was chosen to be applied on **edx** data to calculate the parameters of the model. In last step this model was evaluated by calculating RMSE(residual mean squared error) on the **validation** set. 
&nbsp;

Following libraries were loaded:

```{r  message=FALSE}
library(dslabs)
library(tidyverse)
library(caret)
library(dplyr)
library(lubridate)
library(ggplot2)
library(gridExtra)
library(data.table)
```




```{r message=FALSE}
#  Code to generate edx data set

ratings <- fread(text = gsub("::", "\t", readLines("ml-10M100K/ratings.dat")),
                 col.names = c("userId", "movieId", "rating", "timestamp"))
movies <- str_split_fixed(readLines("ml-10M100K/movies.dat"), "\\::", 3)


colnames(movies) <- c("movieId", "title", "genres")
movies <- as.data.frame(movies, stringsAsFactors=TRUE) %>%
  mutate(movieId = as.numeric(levels(movieId))[movieId], 
         title = as.character(title),
         genres = as.character(genres))
movielens <- left_join(ratings, movies, by = "movieId")

```

```{r message=FALSE}
#  Code to generate the validation set
set.seed(675)
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(ratings, movies, test_index, temp, movielens, removed)
```

## Data Exploration and Data Processing
The **edx** data set contains 9,000,055 observations and 6 variables represented in 6 total columns. Each row represents one user giving one rating to one specific movie. 
&nbsp;

```{r }
#  Number of variables
ncol(edx)
```

```{r }
#  Number of observarions
nrow(edx)
```

The generated data set **edx** contains no missing data and consists of following variables:&nbsp;

- *movieId* is a numerical variable denotes id's for each movie  &nbsp;
- *title* is a string variable describing the title of a movie with a release year &nbsp;
- *genres* is a categorical variable that represent 19 different genres &nbsp;
- *userId* a numerical variable to identify unique users &nbsp;
- *rating* a numerical variable from 0 to 5 &nbsp;
- *timestamp* represents time when rating was given in seconds since January 1, 1970 &nbsp;

```{r }
summary(edx)
```
&nbsp;

Before using the data for visualization and analysis purposes several steps of data transformation were proceeded. The variable *title*" was split in 2 variables *title* and *release_year*. The variable *timestamp* was transformed into a year format and called *rating_year*. The variable *age_years* was created as a difference between *release_year* and *rating_year*. The original *genres* variable represents a combination of several genres, for the purpose of data exploration this variable was split into 19 distinct genres and was only used to create plots.

&nbsp;

```{r echo = FALSE}
#  Split title and year into separate columns by using regex
edx<-extract(edx, title, c("title", "release_year"), "(.*)\\((\\d{4})\\)$")

#  Convert year character into to an integer
edx<-transform(edx, release_year = as.numeric(release_year))

#  Transform the rating timestamp to datetime year
edx<-transform(edx, rating_year = year(as_datetime(timestamp))) 

#  Create an age_years variable that represents time in years
#  between release time and time when rating was given
edx<-edx%>%mutate(age_years=as.numeric(rating_year)-as.numeric(release_year))%>%filter(age_years>=0)

```

There are no missing values present in the **edx** data set.
```{r}
#  Check missing values
sum(is.na(edx))
```

```{r}
head(edx,10)
```

## Users and Movies 
There are 69878 unique users, 10677 movies and 10407 movie titles in the **edx** data set. &nbsp;
```{r }
#  Number of unique users, movies and movie titles in edx
edx %>%summarize(n_users  = n_distinct(userId),
                 n_movies = n_distinct(movieId),
                 n_title  = n_distinct(title))
```

&nbsp;
```{r warning=FALSE}
#  Top 10 of movies with greatest number of ratings
edx%>%group_by(movieId, title)%>%
  summarise(count = n())%>%arrange(desc(count))%>%
  top_n(10, count)
```
&nbsp;


```{r echo=FALSE}
options(scipen = 999)
p_1<-edx %>% 
  count(movieId) %>% 
  ggplot(aes(n, y=..density..)) + 
  geom_histogram(bins=30, fill=I("blue"), col=I("red"), alpha=.2) +
  scale_x_continuous(trans = "log10") + 
  geom_density(col = "black", lwd=0.2)+
  labs(x="n Movies", y="Proportions of n Ratings")+
  theme(plot.title = element_text(hjust = 0.5))

p_2<-edx %>% 
  count(userId) %>% 
  ggplot(aes(n, y=..density..)) + 
  geom_histogram(bins=30, fill=I("grey"), col=I("red"), alpha=.2) +
  scale_x_continuous(trans = "log10") + 
  geom_density(col = "black", lwd=0.2)+
  labs(x="n Users" , y="Proportion of n Ratings") +
  theme(plot.title = element_text(hjust = 0.5))

p<-grid.arrange(p_1, p_2, nrow = 1)

```
As we can see the distribution of rating count among the number of movies and number of users is skewed. Not all users were equally active in giving rating and some movies received more ratings than other. For this reason movie and user effect were taken into modeling the prediction of rating.

## Distribution of Ratings  
The most given ratings were 4.0 and 3.0. Different numbers of ratings among different ratings indicate that users were more likely to give a rating then they liked a movie. Full ratings outnumbered the half ratings.

```{r echo=FALSE}
#  Number of ratings given each rating categories from 0.5 to 5 stars
edx %>% ggplot(aes(rating)) + geom_bar(fill=I("blue"), col=I("grey"), alpha=.2)+
  scale_x_continuous(breaks=seq(0, 5, by= 0.5))+
  ggtitle("Number of Ratings") +
  labs(x="Ratings", y="n of Ratings")+
  theme(plot.title = element_text(hjust = 0.5))
```
&nbsp;

## Genre
In order to demonstrate the effect of genre the *genres* variable was transformed into a single column per genre. There were 19 unique genres and one category  with no listed genres.
```{r echo=FALSE}
#  Split genres into single columns per genre, rename in edx_genre
edx_genre<-separate_rows(edx, genres, sep = "\\|")
```

```{r warning=FALSE}
#  Number of different genres
edx_genre%>%summarize(genre = n_distinct(genres))
```

&nbsp;

Number of ratings for each genre shows that "Drama", "Comedy" and "Action" were top 3 genres that received the most ratings. "Film-Noir", "Documentaries" and  "IMAX" received the least number of ratings.  
```{r echo=FALSE}
#  Number of ratings for each genre
edx_genre %>% group_by(genres) %>% 
  summarize(count=n())%>%
  ggplot(aes(x= reorder(genres, count), y=count))+
  geom_bar(stat='identity', color="blue")+
  ggtitle("Number of Ratings for each Genre") +
  coord_flip(y=c(0, 5000000))+
  labs(x="Genre", y="n Ratings")+
  geom_text(aes(label= count), hjust=-0.1, size=3) +
  theme(plot.title = element_text(hjust = 0.5))
```
&nbsp;

Number of ratings over years when rating was given for each genre. "Drama" remained a popular choice over years by receiving the most number of positive ratings from 4.0 and above. As it is shown in the plot below, "Drama" and "Comedy" received the highest number of rating 4.0 and above in year 2000, while "Documentary" remained flat over years.
```{r echo=FALSE}
#  Change format from scientific to normal
options(scipen = 999)
edx_genre%>%na.omit() %>% filter(rating>4)%>% mutate(genres = as.factor(genres))%>%
  group_by(genres,rating_year)%>%
  summarise(count=n())%>%filter(count > 1000)%>%
  arrange(desc(count))%>%
  ggplot(aes(rating_year, count, color = genres))+
  geom_line()+
  ggtitle("Number of Ratings over Years of Rating") +
  labs(x="Rating Years", y="n of Ratings") +
  scale_x_continuous(limits=c(1995, 2008, by=1))+
  theme(plot.title = element_text(hjust = 0.5))
```
&nbsp;

Following Box plots show how the ratings for each genre are spread in **edx** data. For example "Film-Noir" shows the highest percentage on positive ratings although this genre received only few ratings compared to other genres. Similarly, "Documentary" is a highly rated genre with not many ratings.

-```{r echo=FALSE}
edx_genre%>%filter(genres  %in% c("Drama”, “Comedy", "Action", "Thriller", "Adventure", "Romance", "Sci-Fi", "Crime", "Fantasy", 
                                  "Children", "Horror", "Mystery", "War", "Animation", "Musical", "Western", "Film-Noir", "Documentary", "IMAX" ))%>%
  mutate(count = n(), avg_rating = mean(rating), se_rating = sd(rating)/sqrt(count))%>%
  ggplot(aes(x = reorder(genres, rating), y = rating))+
  geom_boxplot()+
  ggtitle("Boxplot by Genre") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))+
  xlab(" ")+
  ylab("Rating")+
  theme(plot.title = element_text(hjust = 0.5))
  

-```
&nbsp;

```{r echo=FALSE}
rm(edx_genre)
```

## Time 

There are 3 different variables defining time in the **edx** data set:  *rating_year*, *release_year* and *age_years*. Over the years the average rating remained over 3.0 ranging around rating 3.5.

```{r echo=FALSE, warning= FALSE}
#  Average rating over years when rating was given

edx %>% group_by(rating_year) %>%
  summarize(avg_rating = mean(rating)) %>%
  ggplot(aes(rating_year, avg_rating)) +
  geom_point() + geom_smooth()+
  scale_y_continuous(breaks=seq(0, 5, by= 0.5))+
  scale_x_continuous(breaks=seq(1995, 2018, by= 5))+
  ggtitle("Average Rating over Rating Year") +
  labs(x="Rating Year" , y="Average Rating") +
  theme(plot.title = element_text(hjust = 0.5))
```


In comparison between years when movie was first released with years when rating was given. Movies released long time ago don't contain enough data point and show higher variability. 

```{r echo=FALSE, warning= FALSE}
#  Average rating over release years
edx%>%group_by(release_year)%>%
  summarize(avg_rating = mean(rating))%>%
  ggplot(aes(release_year, avg_rating))+
  geom_point() + geom_smooth()+
  ggtitle("Average Rating over Release Years") +
  labs(x="Release Year" , y="Average Rating") +
  theme(plot.title = element_text(hjust = 0.5))

```

&nbsp;

The average rating went up with time past the release date but dropped again for movies older than 70 years.
```{r echo=FALSE, warning= FALSE}
#  Average rating over the age of movies
edx%>%group_by(age_years) %>% summarize(avg_rating = mean(rating))%>%arrange(desc(avg_rating))%>%
  ggplot(aes(age_years, avg_rating)) +
  scale_x_continuous(breaks=seq(0, 100, by= 10))+
  geom_point() + geom_smooth()+
  ggtitle("Average Rating over Movie Age") +
  labs(x="Age of Movie" , y="Average Rating") +
  theme(plot.title = element_text(hjust = 0.5))
```
In the Method part all the variables described above were gradually integrated in the models in order to predict ratings with the decreasing value of RMSE  . 
&nbsp;

# Method 

It was important NOT to use the **validation** set to train the algorithm. The **edx** data was split into a **train_set** and a **test_set**, where the test_set was set to 20% of the **edx** data. The **train_set** was used to train all model, while **test_set** was used to estimate predictions of ratings and to calculate the RMSE in order to evaluate the model.
&nbsp;

```{r}
set.seed(110)
test_index <- createDataPartition(y=edx$rating, times = 1, p = 0.2, list = FALSE)
test_set <- edx[test_index, ]
train_set <- edx[-test_index, ]  
```
semi_join function removes entries for users and movies in test_set that dont's appear in th train_set.

`````{r}
test_set <- test_set %>% 
  semi_join(train_set, by = "movieId") %>%
  semi_join(train_set, by = "userId")
````


There were 7 possible predictors of rating: *genres*, *release_year*, *rating_year*, *movieId*, *userId*, *title*, *age_years*. Models based on one and a combination of several predictors were tested on the **train_set**. The **test_set** was used to calculate RMSE of each model as it was done in the *Netflix challenge*. RMSE's of multiple models were compared with each other. After the comparison model that yielded the smallest RMSE was used for the final evaluation on the **validation** set to test the final algorithm. **RMSE < 0.86490** was considered acceptable.
&nbsp;

The RMSE (residual mean squared error) is defined as:
$RMSE = \sqrt{\frac{1}{N}\sum_{u,m}(\hat y_{u,m}-y_{u,m})^2}$ 
&nbsp;

Where $y_{u,m}$ is observed rating by user $u$ for movie $m$ and $\hat y_{u,m}$ is the prediction of the rating. $N$ is a number of movies and users.
&nbsp;

Following code for RMSE was used
```{r }
RMSE <- function(true_ratings, predicted_ratings){
  sqrt(mean((true_ratings-predicted_ratings)^2, na.rm = T))
}
```

Methods described here are based on the book by Rafael A. Irizarry[^3]. First the simplest model with movie effects was applied. 

$$y_{u,m} = \mu + b_{m} + \epsilon_{u,m}$$
&nbsp;

Where $b_{m}$ is a movie effect, $y_{u,m}$ observed rating and $\epsilon_{u,m}$ independent errors. This model was extended with additional effects such as  $b_{u}$ - user effect, $b_{g}$ - genre effect and $b_{a}$ - age of movie effect.


(M1) Model with movie effects: $\hat{b_{m}}$ estimated as an average of $y_{u,m} - \hat{\mu}$, predicted ratings $\hat y_{u,m}= \hat{\mu} + \hat{b_{m}}$
&nbsp;

(M2) Model with movie and user effects: $\hat{b_{u}}$ estimated as an average of $y_{u,m} - \hat{\mu} - \hat{b_m}$, predicted ratings $\hat y_{u,m}= \hat{\mu} + \hat{b_{m}} + \hat{b_{u}}$ 
&nbsp;

(M3) Model with movie, user  and genre effects: $\hat{b_{g}}$ estimated as an average of $y_{u,m} - \hat{\mu} - \hat{b_m} - \hat{b_u}$, predicted ratings $\hat y_{u,m}= \hat{\mu} + \hat{b_{m}} + \hat{b_{u}} + \hat{b_{g}}$ 

&nbsp;

(M4) Model with movie, user, genre and age effects: $\hat{b_{a}}$ estimated as an average of $y_{u,m} - \hat{\mu} - \hat{b_m} - \hat{b_u} - \hat{b_g}$, predicted ratings $\hat y_{u,m}= \hat{\mu} + \hat{b_{m}} + \hat{b_{u}} + \hat{b_{g}} + \hat{b_{a}}$ 

&nbsp;

In order to limit the variability of this effect a penalty term $\lambda$ which minimizes effects for the small sample sizes and stabilizes big samples. 

(M5) Model with the regularized movie effects: $\hat{b_{m}} = {\frac{1}{\lambda + n_{m}} }\sum_{n=1}^{n_m}(y_{u,m}-\hat{\mu})$

&nbsp;

(M6) Model with the regularized regularized movie and user effects: $\hat{b_{u}} = {\frac{1}{\lambda + n_{u}} }\sum_{n=1}^{n_u}(y_{u,m}-\hat{\mu} -\hat{b_{m}})$

&nbsp;

(M7) Model with the regularized movie, user and genre effects: $\hat{b_{g}} = {\frac{1}{\lambda + n_{g}} }\sum_{n=1}^{n_g}(y_{u,m}-\hat{\mu} -\hat{b_{m}} - \hat{b_{u}})$

&nbsp;

(M8) Model with the regularized movie, user, genre and age effects: $\hat{b_{a}} = {\frac{1}{\lambda + n_{a}} }\sum_{n=1}^{n_a}(y_{u,m}-\hat{\mu} -\hat{b_{m}} - \hat{b_{u}} - \hat{b_{g} })$ 

&nbsp;


# Results 
The results of the models (M1) - (M4) are presented:
```{r warning= FALSE}
# # # # # # # # # # # # # # # # # # # # 
#  (M1) Model with movie effects
# # # # # # # # # # # # # # # # # # # # 

# Average rating 
mu <- mean(train_set$rating)

# Estimate movie effect b_m
movie_ef <- train_set %>%
  group_by(movieId) %>%
  summarize(b_m = mean(rating - mu))

#  Estimate predicted ratings
predicted_ratings <- test_set %>%
  left_join(movie_ef, by='movieId') %>%
  mutate(pred = mu + b_m) %>%
  pull(pred)

#  Calculate RMSE of Model 1
rmes_1<-RMSE(predicted_ratings, test_set$rating)

rmse_results <- data_frame(Model = "(M1) Movie Effects", RMSE = rmes_1)
rmse_results%>% knitr::kable()
```


```{r warning= FALSE}
# # # # # # # # # # # # # # # # # # # # # # # 
#  (M2) Model with movie and user effects
# # # # # # # # # # # # # # # # # # # # # # # 

# Average rating
mu <- mean(train_set$rating)

# Estimate movie effect b_m
movie_ef <- train_set %>%
  group_by(movieId) %>%
  summarize(b_m = mean(rating - mu))
# Estimate user effect b_u
user_ef <- train_set %>%
  left_join(movie_ef, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_m))
# Estimate predicted rating
predicted_ratings <- test_set %>%
  left_join(movie_ef, by='movieId') %>%
  left_join(user_ef,  by='userId') %>%
  mutate(pred = mu + b_m + b_u) %>%
  pull(pred)

# Calculate RMSE of Model 2
rmse_2<-RMSE(predicted_ratings, test_set$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="(M2) Movie and User Effects Model",
                                     RMSE = rmse_2))

rmse_results%>% knitr::kable()
```

```{r warning= FALSE}
# # # # # # # # # # # # # # # # # # # # # # # # # # 
#  (M3) Model with movie, user and genre effects
# # # # # # # # # # # # # # # # # # # # # # # # # # 

#  Average rating
mu <- mean(train_set$rating)
#  Estimate movie effect b_m
movie_ef <- train_set %>%
  group_by(movieId) %>%
  summarize(b_m = mean(rating - mu))%>%ungroup()
#  Estimate user effect b_u
user_ef <- train_set %>%
  left_join(movie_ef, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_m)) 
#  Estimate genre effect b_g
genre_ef<-train_set%>%
  left_join(movie_ef, by="movieId")%>%
  left_join(user_ef, by="userId")%>%
  group_by(genres)%>%
  summarize(b_g = mean(rating - mu - b_m- b_u)) 

#  Estimate predicted ratings
predicted_ratings <- test_set %>%
  left_join(movie_ef, by='movieId') %>%
  left_join(user_ef,  by='userId') %>%
  left_join(genre_ef, by='genres') %>%
  mutate(pred = mu + b_m + b_u + b_g) %>%
  pull(pred)

#  Calculate RMSE of Model 3
rmse_3<-RMSE(predicted_ratings, test_set$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="(M3) Movie, User and Genre Effects Model",
                                     RMSE = rmse_3))
rmse_results%>% knitr::kable()
```


```{r warning= FALSE}
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 
#  (M4) Model with movie, user, genre and age effects
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 

#  Average rating
mu <- mean(train_set$rating)
# Estimate movie effect b_m
movie_ef <- train_set %>%
  group_by(movieId) %>%
  summarize(b_m = mean(rating - mu))
#  Estimate user effect b_u
user_ef <- train_set %>%
  left_join(movie_ef, by='movieId') %>%
  group_by(userId) %>%
  summarize(b_u = mean(rating - mu - b_m))
#  Estimate genre effect b_g
genre_ef<-train_set%>%
  left_join(movie_ef, by="movieId")%>%
  left_join(user_ef, by="userId")%>%
  group_by(genres)%>%
  summarize(b_g = mean(rating - mu - b_m- b_u))
#  Estimate age effect b_a
age_ef<-train_set%>%
  left_join(movie_ef, by="movieId")%>%
  left_join(user_ef,  by="userId")%>%
  left_join(genre_ef, by="genres") %>%
  group_by(age_years)%>%
  summarize(b_a = mean(rating - mu - b_m - b_u - b_g))
# Estimate predicted ratings
predicted_ratings <- test_set %>%
  left_join(movie_ef, by='movieId') %>%
  left_join(user_ef,  by='userId') %>%
  left_join(genre_ef, by='genres') %>%
  left_join(age_ef,   by ="age_years")%>%
  mutate(pred = mu + b_m + b_u + b_g  + b_a) %>%
  pull(pred)

rmse_4<-RMSE(predicted_ratings, test_set$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="(M4) Movie, User, Genre and Age Effects Model",
                                     RMSE = rmse_4))

rmse_results%>% knitr::kable()
```
In this part results of the models (M4) - (M8) with the regularized effects are presented. First the optimal (with smallest RMSE) tuning parameter $\lambda$ was chosen.

```{r echo=FALSE, warning= FALSE}

# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 
#  (M5) Model with the regularized movie effect with lambda
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 

#  Choosing the tuning parameter lambda 
lambda <- seq(0, 15, 0.50)

rmses_m_ef_reg <- sapply(lambda, function(l){
  
  mu <- mean(train_set$rating)
  
  b_m <- train_set %>%
    group_by(movieId) %>%
    summarize(b_m = sum(rating - mu)/(n()+l), n_m = n())
  
  predicted_ratings <-test_set %>%
    left_join(b_m, by = "movieId") %>%
    mutate(pred = mu + b_m) %>% pull(pred)
  
  rmses_m_ef_reg<-return(RMSE(predicted_ratings, test_set$rating))
})
```

```{r echo = FALSE}
qplot(lambda, rmses_m_ef_reg)

```

```{r warning= FALSE}
#  Choose tuning parameter l_1 with minimal RMSE
l_1<-lambda[which.min(rmses_m_ef_reg)]
l_1
# Average rating  
mu <- mean(train_set$rating)
# Estimate movie effect b_m
movie_ef_reg <- train_set %>%
  group_by(movieId) %>%
  summarize(b_m = sum(rating - mu)/(n()+l_1), n_m = n())
# Estimate predicted ratings
predicted_ratings <- test_set %>%
  left_join(movie_ef_reg, by = "movieId") %>%
  mutate(pred = mu + b_m) %>%
  pull(pred)
# Calculate RMSE of Model 5
rmse_5<-RMSE(predicted_ratings, test_set$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="(M5) Regularized Movie Effects Model",
                                     RMSE = rmse_5))

rmse_results%>% knitr::kable()
```

```{r echo=FALSE, warning= FALSE}
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #  
#  (M6) Model with the regularized regularized movie + user effect with tuning parameter lambda
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 

#  Choose tuning parameter lambda
lambda <- seq(0, 15, 0.50)

rmses_mu_ef_reg <- sapply(lambda, function(l){
  
  mu <- mean(train_set$rating)
  
  b_m <- train_set %>%
    group_by(movieId) %>%
    summarize(b_m = sum(rating - mu)/(n()+l), n_m = n())
  
  b_u <- train_set %>%
    left_join(b_m, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_m - mu)/(n()+l), n_u = n())
  
  predicted_ratings <-test_set %>%
    left_join(b_m, by = "movieId") %>%
    left_join(b_u, by = "userId") %>% 
    mutate(pred = mu + b_m + b_u) %>%
    pull(pred)
  
  rmses_mu_ef_reg<-return(RMSE(predicted_ratings, test_set$rating))
})

```

```{r echo = FALSE}
qplot(lambda, rmses_mu_ef_reg)
```

```{r warning= FALSE}
#  tuning parameter l_2 with minimal RMSE 
l_2<-lambda[which.min(rmses_mu_ef_reg)]
l_2
# Average rating
mu <- mean(train_set$rating)
# Estimate movie effect b_m
movie_ef_reg <- train_set %>%
  group_by(movieId) %>%
  summarize(b_m = sum(rating - mu)/(n()+l_2), n_m= n())
# Estimate user effect b_u 
user_ef_reg <- train_set %>%
  left_join(movie_ef_reg, by='movieId') %>% 
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_m - mu)/(n() +l_2), n_u = n())
#  Estimate predicted ratings 
predicted_ratings <- test_set %>%
  left_join(movie_ef_reg, by = "movieId") %>%
  left_join(user_ef_reg, by = "userId") %>%
  mutate(pred = mu + b_m + b_u) %>%
  pull(pred)

# Calculate RMSE of Model 6
rmse_6<-RMSE(predicted_ratings, test_set$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="(M6) Regularized Movie and User Effects Model",
                                     RMSE = rmse_6))

rmse_results%>% knitr::kable()
```


```{r echo=FALSE, warning= FALSE}
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 
#  (M7) Model with the regularized movie + user + genre effect with tuning parameter lambda
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 

#  Choose tuning parameter lambda 
lambda <- seq(0, 15, 0.50)

rmses_mug_ef_reg <- sapply(lambda, function(l){
  
  mu <- mean(train_set$rating)
  
  b_m <- train_set %>%
    group_by(movieId) %>%
    summarize(b_m = sum(rating - mu)/(n()+l), n_m = n())
  
  b_u <- train_set %>%
    left_join(b_m, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_m - mu)/(n()+l), n_u = n())
  
  b_g <- train_set %>%
    left_join(b_m, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    group_by(genres)%>%
    summarize(b_g = sum(rating - b_m - b_u - mu)/(n()+l), n_g = n())
  
  predicted_ratings <-test_set %>%
    left_join(b_m, by = "movieId") %>%
    left_join(b_u, by = "userId") %>% 
    left_join(b_g, by = "genres") %>% 
    mutate(pred = mu + b_m + b_u +b_g) %>%
    pull(pred)
  
  rmses_mug_ef_reg<-return(RMSE(predicted_ratings, test_set$rating))
})

```


```{r echo = FALSE}
qplot(lambda, rmses_mug_ef_reg)

```

```{r warning= FALSE}
#  tuning parameter l_3 with minimal RMSE 
l_3<-lambda[which.min(rmses_mug_ef_reg)]
l_3
#  Average rating
mu <- mean(train_set$rating)
# Estimate movie effect b_m
movie_ef_reg <- train_set %>%
  group_by(movieId) %>%
  summarize(b_m = sum(rating - mu)/(n()+l_3), n_m= n())
# Estimate user effect b_u
user_ef_reg <- train_set %>%
  left_join(movie_ef_reg, by='movieId') %>% 
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_m - mu)/(n() +l_3), n_u = n())
#  Estimate genre effect b_g
genre_ef_reg <- train_set %>%
  left_join(movie_ef_reg, by="movieId") %>%
  left_join(user_ef_reg, by="userId") %>%
  group_by(genres)%>%
  summarize(b_g = sum(rating - b_m - b_u - mu)/(n()+l_3), n_g = n())
#  Estimate predicted ratings 
predicted_ratings <- test_set %>%
  left_join(movie_ef_reg, by = "movieId") %>%
  left_join(user_ef_reg, by = "userId") %>%
  left_join(genre_ef_reg, by="genres")%>%
  mutate(pred = mu + b_m + b_u + b_g) %>%
  pull(pred)

# Calculate RMSE of Model 7
rmse_7<-RMSE(predicted_ratings, test_set$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="(M7) Regularized Movie, User and Genre Effects Model",
                                     RMSE = rmse_7))

rmse_results%>% knitr::kable()
```


```{r echo=FALSE, warning= FALSE}
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 
#  (M8) Model with the regularized movie + user + genre + age effect with tuning parameter lambda
# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # 

#  Choose tuning parameter lambda 
lambda <- seq(0, 15, 0.50)

rmses_muga_ef_reg <- sapply(lambda, function(l){
  
  mu <- mean(train_set$rating)
  
  b_m <- train_set %>%
    group_by(movieId) %>%
    summarize(b_m = sum(rating - mu)/(n()+l), n_m = n())
  
  b_u <- train_set %>%
    left_join(b_m, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_m - mu)/(n()+l), n_u = n())
  
  b_g <- train_set %>%
    left_join(b_m, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    group_by(genres)%>%
    summarize(b_g = sum(rating - b_m - b_u - mu)/(n()+l), n_g = n())
  
  b_a <- train_set %>%
    left_join(b_m, by="movieId") %>%
    left_join(b_u, by="userId") %>%
    left_join(b_g, by="genres") %>%
    group_by(age_years)%>%
    summarize(b_a = sum(rating - b_g - b_m - b_u - mu)/(n()+l), n_a = n())
  
  predicted_ratings <-test_set %>%
    left_join(b_m, by = "movieId") %>%
    left_join(b_u, by = "userId") %>% 
    left_join(b_g, by = "genres") %>%
    left_join(b_a, by = "age_years")%>%
    mutate(pred = mu + b_m + b_u + b_g + b_a) %>%
    pull(pred)
  
  rmses_muga_ef_reg<-return(RMSE(predicted_ratings, test_set$rating))
})
```

```{r echo = FALSE}
qplot(lambda, rmses_muga_ef_reg)

```

```{r warning= FALSE}
#  tuning parameter l_4 with minimal RMSE 
l_4<-lambda[which.min(rmses_muga_ef_reg)]
l_4
#  Average rating
mu <- mean(train_set$rating)
# Estimate movie effect b_m
movie_ef_reg <- train_set %>%
  group_by(movieId) %>%
  summarize(b_m = sum(rating - mu)/(n()+l_4), n_m= n())
# Estimate user effect b_u
user_ef_reg <- train_set %>%
  left_join(movie_ef_reg, by='movieId') %>% 
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_m - mu)/(n() +l_4), n_u = n())
#  Estimate genre effect b_g
genre_ef_reg <- train_set %>%
  left_join(movie_ef_reg, by = "movieId") %>%
  left_join(user_ef_reg,  by = "userId") %>%
  group_by(genres)%>%
  summarize(b_g = sum(rating - b_m - b_u - mu)/(n()+l_4), n_g = n())
# Estimate age effect b_a
age_ef_reg <- train_set %>%
  left_join(movie_ef_reg, by = "movieId") %>%
  left_join(user_ef_reg,  by = "userId") %>%
  left_join(genre_ef_reg, by = "genres")%>%
  group_by(age_years)%>%
  summarize(b_a = sum(rating - b_g - b_m - b_u - mu)/(n()+l_4), n_a = n())
#  Estimate predicted ratings 
predicted_ratings <- test_set %>%
  left_join(movie_ef_reg, by = "movieId") %>%
  left_join(user_ef_reg,  by = "userId") %>%
  left_join(genre_ef_reg, by = "genres")%>%
  left_join(age_ef_reg,   by = "age_years")%>%
  mutate(pred = mu + b_m + b_u + b_g + b_a) %>%
  pull(pred)

# Calculate RMSE of Model 8
rmse_8<-RMSE(predicted_ratings, test_set$rating)

rmse_results <- bind_rows(rmse_results,
                          data_frame(Model="(M8) Regularized Movie, User, Genre and Age Effects Model",
                                     RMSE = rmse_8))

rmse_results%>% knitr::kable()
```

Since the smallest **RMSE = 0.8648** was produced by (M8) Regularized Movie, User, Genre and Age Effects Model, this model was used on the **edx** to train the algorithm and on the **validation** data to test and to calculate the final RMSE.
&nbsp; 

``` {r message=FALSE}
# # # # # # # # # # # 
#  Final Model
# # # # # # # # # # # 

# Validation Data processing

#  Split title and year into separate columns by using regex
validation<-extract(validation, title, c("title", "release_year"), "(.*)\\((\\d{4})\\)$")
#  Convert year character into to an integer
validation<-transform(validation, release_year = as.numeric(release_year))
#  Transform the rating timestamp to datetime year
validation<-transform(validation, rating_year = year(as_datetime(timestamp)))
#  Create an age_years variable that represents time in years
#  between release time and time when rating was given
validation<-validation%>%mutate(age_years=as.numeric(rating_year)-as.numeric(release_year))%>%filter(age_years>=0)
```

```{r warning= FALSE}
#  tuning parameter l_4 with minimal RMSE 
l_4<-lambda[which.min(rmses_muga_ef_reg)]
l_4
#  Average rating
mu <- mean(edx$rating)
# Estimate movie effect b_m
movie_ef_reg <- edx %>%
  group_by(movieId) %>%
  summarize(b_m = sum(rating - mu)/(n()+l_4), n_m= n())
# Estimate user effect b_u
user_ef_reg <- edx %>%
  left_join(movie_ef_reg, by='movieId') %>% 
  group_by(userId) %>%
  summarize(b_u = sum(rating - b_m - mu)/(n() +l_4), n_u = n())
#  Estimate genre effect b_g
genre_ef_reg <- edx %>%
  left_join(movie_ef_reg, by = "movieId") %>%
  left_join(user_ef_reg,  by = "userId") %>%
  group_by(genres)%>%
  summarize(b_g = sum(rating - b_m - b_u - mu)/(n()+l_4), n_g = n())
# Estimate age effect b_a
age_ef_reg <- edx %>%
  left_join(movie_ef_reg, by = "movieId") %>%
  left_join(user_ef_reg,  by = "userId") %>%
  left_join(genre_ef_reg, by = "genres")%>%
  group_by(age_years)%>%
  summarize(b_a = sum(rating - b_g - b_m - b_u - mu)/(n()+l_4), n_a = n())
#  Estimate predicted ratings 
predicted_ratings <- validation %>%
  left_join(movie_ef_reg, by = "movieId") %>%
  left_join(user_ef_reg,  by = "userId") %>%
  left_join(genre_ef_reg, by = "genres")%>%
  left_join(age_ef_reg,   by = "age_years")%>%
  mutate(pred = mu + b_m + b_u + b_g + b_a) %>%
  pull(pred)

# Calculate RMSE of Final Model
rmse_final<-RMSE(predicted_ratings, validation$rating)

rmse_results <- bind_rows(rmse_results,
                          tibble(Model="Final Regularized Movie, User, Genre and Age Effects Model",
                                 RMSE = rmse_final))

rmse_results%>% knitr::kable()
```
The final **RMSE= 0.8629**

# Conclusion 
The algorithm was developed by first splitting the **edx** into **test_ set** and **train_set** set. In the last step algorithm, predict movie ratings in the validation set as if they were unknown. The acceptable RMSE was reached by applying "Regularized Movie, User, Genre and Age Effects Model".  While working on this project the main difficulty was the the memory capacity which lead to the reduction of the visualization and analysis techniques The calculations were run on AWS(Amazon Web Services).

[^1]: http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/ 
[^2]: https://grouplens.org/
[^3]: https://rafalab.github.io/dsbook/
